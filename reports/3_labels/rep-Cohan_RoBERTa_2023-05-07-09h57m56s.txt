RESULTS REPORT - Cohan
Model: RoBERTa
Encoder: roberta-base
Dataset: facts
Chunk layout: Cohan
Evaluation: test set (5 random seeds)
Max sequence length: 512
Max sentence length: 85
Max sentences per chunk: 7
Batch size: 16
Dropout rate: 0.2
Learning rate: 1e-05
Adam Epsilon: 1e-08
Weight decay: 0.001
Train time: 00h32m30s
GPU name: Tesla V100-SXM2-16GB
GPU memory: 15.78

Averages:
Epoch Train loss   std    Test loss   std    P (macro) P std  R (macro) R std  F1 (macro) F1 std
  1    0.529112  0.023353  0.313819 0.028116   0.6152  0.1058   0.5681  0.0443   0.5817   0.0594
  2    0.382841  0.014512  0.285644 0.010887   0.7997  0.0549   0.6618  0.0545   0.6883   0.0345
  3    0.315323  0.008617  0.267066 0.008588   0.8208  0.0380   0.6349  0.0248   0.6825   0.0208
  4    0.277655  0.008327  0.265873 0.012967   0.7850  0.0184   0.6734  0.0246   0.7110   0.0191

*** Detailed report ***

Confusion matrices
------------------
Fact: 0 
RulingByPresentCourt: 1 
Other: 2 
=> Iteration 0:
Epoch 1:
[[ 292    0  100]
 [   1    0   36]
 [  94    0 1674]]
Epoch 2:
[[ 305    0   87]
 [   1   10   26]
 [ 112    6 1650]]
Epoch 3:
[[ 264    0  128]
 [   1   13   23]
 [  65   11 1692]]
Epoch 4:
[[ 304    0   88]
 [   1   15   21]
 [ 109   10 1649]]
=> Iteration 1:
Epoch 1:
[[ 218    0  174]
 [   1    0   36]
 [  52    0 1716]]
Epoch 2:
[[ 249    0  143]
 [   1    6   30]
 [  71    0 1697]]
Epoch 3:
[[ 283    0  109]
 [   2    8   27]
 [ 113    1 1654]]
Epoch 4:
[[ 273    0  119]
 [   1   11   25]
 [ 106    4 1658]]
=> Iteration 2:
Epoch 1:
[[ 269    0  123]
 [   1   11   25]
 [  79    3 1686]]
Epoch 2:
[[ 271    0  121]
 [   1   23   13]
 [  78   22 1668]]
Epoch 3:
[[ 247    0  145]
 [   1    7   29]
 [  57    1 1710]]
Epoch 4:
[[ 269    0  123]
 [   1   17   19]
 [  72    8 1688]]
=> Iteration 3:
Epoch 1:
[[ 268    0  124]
 [   1    0   36]
 [  52    0 1716]]
Epoch 2:
[[ 314    0   78]
 [   3   11   23]
 [ 123    5 1640]]
Epoch 3:
[[ 257    0  135]
 [   3   10   24]
 [  52    3 1713]]
Epoch 4:
[[ 295    0   97]
 [   4   11   22]
 [  89    4 1675]]
=> Iteration 4:
Epoch 1:
[[ 308    0   84]
 [   3    0   34]
 [ 132    0 1636]]
Epoch 2:
[[ 293    0   99]
 [   4    8   25]
 [ 101    2 1665]]
Epoch 3:
[[ 280    0  112]
 [   4   12   21]
 [  89    4 1675]]
Epoch 4:
[[ 255    0  137]
 [   4   13   20]
 [  68    8 1692]]

Scores
------
Epoch: 1
             Train loss  Test loss  P (macro)  R (macro)  F1 (macro)
Iteration 0    0.507764   0.329529   0.559795   0.563910    0.561799
Iteration 1    0.573835   0.305082   0.565131   0.508904    0.528897
Iteration 2    0.528874   0.303991   0.825263   0.645714    0.697855
Iteration 3    0.516268   0.273242   0.583201   0.551421    0.564525
Iteration 4    0.518818   0.357250   0.542662   0.570351    0.555581

Epoch: 2
             Train loss  Test loss  P (macro)  R (macro)  F1 (macro)
Iteration 0    0.367312   0.289715   0.763523   0.660530    0.688341
Iteration 1    0.408768   0.276289   0.894396   0.585736    0.636819
Iteration 2    0.382411   0.272873   0.737012   0.752129    0.741963
Iteration 3    0.371158   0.285659   0.781041   0.675307    0.701547
Iteration 4    0.384556   0.303684   0.822289   0.635136    0.672793

Epoch: 3
             Train loss  Test loss  P (macro)  R (macro)  F1 (macro)
Iteration 0    0.310995   0.268397   0.753245   0.660611    0.698223
Iteration 1    0.331211   0.279476   0.841322   0.624558    0.664673
Iteration 2    0.310134   0.271574   0.864160   0.595495    0.652112
Iteration 3    0.317281   0.254325   0.836004   0.631591    0.690441
Iteration 4    0.306995   0.261556   0.809036   0.662003    0.707219

Epoch: 4
             Train loss  Test loss  P (macro)  R (macro)  F1 (macro)
Iteration 0    0.269698   0.288872   0.757432   0.704536    0.724517
Iteration 1    0.292680   0.260493   0.790614   0.643836    0.686394
Iteration 2    0.279903   0.261550   0.796318   0.700145    0.739885
Iteration 3    0.275146   0.249852   0.809103   0.665749    0.706657
Iteration 4    0.270850   0.268599   0.771318   0.652958    0.697725

